\chapter{相关文献综述}
\label{chapter:background}

\section{本章引言}
数据中心中有一个持续火热的话题：
如何使用廉价、常见的网络设备来给数据中心应用提供提供低延迟，高带宽服务。
在数据中心使用TCP作为传输策略会导致传输效率低下，用户体验差的问题。
因此，许多新型方案被提出。
本章综述了数据中心新型传输方案。
首先，本章从常见应用的通信模型和数据流等方面对数据中心传输进行概述。
然后，本章从流级别和任务级别对数据中心传输方案进行综述。
最后，本章总结数据中心传输方案存在的问题。


\section{数据中心应用传输概述}
在过去的几年中，数据中心随着在线应用的迅速发展取得了巨大的发展。
越来越多的企业有了自己的数据中心，还出现了比如Amazon，微软和谷歌的这些数据中心服务提供商。
在数据中心中有一个持续火热的话题：
如何使用廉价、常见的网络设备来给数据中心应用提供提供低延迟、高带宽服务。
尽管数据中心中存在网络搜索，购物，广告等这些在线应用，这些应用也扮演者不同的角色，
但是，这些应用在使用的编程模型，传输延迟，数据流特征等方面有一些共同的特征。
总结这些特征，可以设计高效的网络传输策略。

\subsection{分布式应用的编程模型}

\begin{figure}[h]
\setlength{\abovecaptionskip}{0pt} 
\setlength{\belowcaptionskip}{1pt} 
  \centering
  \subcaptionbox{Map-reduce}
    {\includegraphics[width=0.48\columnwidth]{figures/others/map-reduce.pdf}}
  \subcaptionbox{带有阻碍的数据流}
      {\includegraphics[width=0.48\columnwidth]{figures/others/vertex2.pdf}}
  \subcaptionbox{没有显式阻碍的数据流}
    {\includegraphics[width=0.48\columnwidth]{figures/others/vertex.pdf}}
  \subcaptionbox{环形数据流}
      {\includegraphics[width=0.48\columnwidth]{figures/others/Dataflow.pdf}}
   \subcaptionbox{整体同步并行计算}
      {\includegraphics[width=0.48\columnwidth]{figures/others/BSP.pdf}}
   \subcaptionbox{分区－聚合}
      {\includegraphics[width=0.48\columnwidth]{figures/others/partition-aggregate.pdf}}
  \caption{分布式应用的通信模型}
  \label{relatedwork-communication-fig}
\end{figure}

分布式应用程序的计算框架都是输入特定的配置参数，然后经过多个节点的不同阶段计算，最后得到结算结果。
计算过程中，常常产生大量的数据，节点之间会进行频繁通信，不同的编程模型有不同的通信特征。
图\ref{relatedwork-communication-fig}显示了6种编程模型，这些计算模型可以分成4类：
映射规约（Map-Reduce）模型，数据流（Dataflow）模型，
整体同步并行计算（Bulk Synchronous Parallel，简称BSP )模型，分区－聚合（Partition-Aggregate）模型。

\subsubsection{映射规约（Map-Reduce）模型}
MapReduce\cite{Dean2004Simplified}是一个众所周知的集群编程模型。
如图\ref{relatedwork-communication-fig}(a)所示，
每个mapper都从分布式文件系统(DFS)中读取输入，执行用户定义的计算，并将中间数据写到磁盘上;
每个reducer都从不同的mapper中提取中间数据，将它们合并，并将其输出写入分布式文件系统。

给定m个mappers和r个reducer，MapReduce的shuffle阶段将产生总数为$m\times r$条数据流。
经过reducer处理完毕以后，至少有r条数据流被复制。
MapReduce模型中shuffle阶段会产生很多并行的数据流，
直到最后一条数据流传输结束此传输过程才会传输完毕。
因此，在任务传输的最后阶段，有一个显式的同步。
有研究\cite{Chowdhury2011Managing}针对MapReduce存在任务数据流同步的特点，
优化了shuffle阶段的传输，以此来提高传输的性能。

\subsubsection{数据流（Dataflow）模型}
虽然MapReduce被广泛使用，但是因为其存在数据阻塞，并且传统Map-Reduce只能处理单一的流，常常会影响应用的整体性能。
Dataflow的出现解决了MapReduce效率低下的问题，并且可以构建整个pipeline。
Dataflow计算模型模型分成以下三种方式：

\textbf{带有阻塞的数据流模型}。
如图\ref{relatedwork-communication-fig}(b)所示，
有的多阶段的数据流模型使用MapReduce作为其构建模块（例如，Hive\footnote{Apache Hive.~http://hadoop.apache.org/hive/.}，Pig\cite{Olston2008Pig})。
因为此计算模型是基于MapReduce，因而和MapReduce类似，
带有阻塞的Dataflow的通信模型在每个构建模块的末尾都有同步操作。

\textbf{无阻塞的数据流模型}。
如图\ref{relatedwork-communication-fig}（c）所示，
一些数据流模型并没有明显的同步操作(比如Dryad\cite{Isard2007Dryad}，DryadLINQ\cite{Yu2008DryadLINQ}，SCOPE\cite{Chaiken2008SCOPE}，FlumeJava\cite{Chambers2010FlumeJava})。
当输入的数据准备好以后，下一个阶段就会开始。
因为没有明确的同步操作，
所以针对MapReduce等存在同步框架的优化技术对此通信框架没有任何效果。
很多研究人员针对采用没有显式阻塞的数据流通信模式的应用进行了特殊优化处理\cite{Guo2015Spotting,Zhang2012Optimizing}。

\textbf{环形数据流模型}。
如图\ref{relatedwork-communication-fig}（d）所示的是环形数据流通信模型。
和无阻塞的数据流模型不同的是，在环形数据流中，数据流会重复使用之前阶段的部分组件。
因为不同的阶段的数据流传输会复用组件，因此，可能会引起组件使用的冲突。
Spark\cite{Zaharia2012Resilient}通过在迭代中保留内存状态来消除循环冲突带来的开销。

\subsubsection{整体同步并行计算（Bulk Synchronous Parallel )模型}
整体同步并行计算（Bulk Synchronous Parallel，简称BSP )模型是集群计算中的另一个众所周知的通信模型。
使用这种模型的框架包括Pregel\cite{Malewicz2010Pregel}、
Giraph\footnote{Apache Giraph.~http://incubator.apache.org/giraph/.}和Hama\footnote{Apache Hama.~http://hama.apache.org.}等，
许多图形处理、矩阵计算等工具使用BSP框架。
如图\ref{relatedwork-communication-fig}(e)所示，
BSP计算在一系列全局superstep中进行，
每个superstep包含三个有序阶段：并行计算、进程通信和同步数据。
在每一个superstep的末尾有明确的同步操作。

\subsubsection{分区－聚合（Partition-Aggregate）模型}
图\ref{relatedwork-communication-fig}(f)显示是数据中心分区－聚合（Partition-Aggregate）模型。
分区－聚合模型是很多大型分布式计算通信模型。
在分区－聚合模型中，请求发送到根节点，根节点把请求下发给底部worker节点。
随后，worker节点把计算完成的结果，反馈给根节点。
在数据中心，网络搜索、社交网络和广告选择等应用的通信模式均是基于此模式设计的。
对于交互式、实时的应用，用户得到响应的延迟是衡量服务质量好坏的重要指标。
在减去计算、存储需要的时间，留给数据传输的时间通常只有230$\sim$300毫秒。

许多应用采用多层的分区－聚合模式，有的应用比如搜索等应用甚至采用多层的分区－聚合模型。
多层的分区－聚合模型中，上层的传输延迟会影响下一层的启动时间，进而影响下一层的延迟。
此外，响应请求可能需要迭代地调用同一层的计算结果，例如社交网络应用中存在一个aggregator节点向下面的worker节点发出连续的请求（1到4次响应是常见的，甚至有时能到20次）。
分区－聚合传输延迟过高会影响查询结果性能。

因此，为了防止SLA（Service Level Agreement）性能被破坏，
常常需要给worker节点的传输设置一个截止期限（deadline），截止期限大小在10ms$\sim$100ms。
如果worker节点的数据流传输错过了截止期限，上层aggregator节点将会收到不完整的结果，这样会使的计算结果不完整，进而使应用性能
受到影响。
因此，对于用户来说，更多的流在截止期限之前完成对用户体验十分重要。
例如，如果有$99.9\%$的流在截止期限之前完成，从而，1000个请求中，有1个请求不能在截止时间之前完成。
这样会严重影响用户的体验，造成客户的流失，造成不可逆转的损失。

综上所属，数据中心分布式应用的计算模型产生的数据流有以下两个特征：
（1）数据流传输存在期限。无论是MapReduce模型，Dataflow模型，BSP模型还是Partition-Aggregate模型，
都需要计算结果在一定的截止期限内传输完成，
否则会导致计算结果的不完整性，进而影响用户性能；
（2）需要进行任务级别优化。在分布式计算中，只进行流级别传输是不够的，只有一个阶段的所有数据流传输完成时，此阶段才算传输完毕。
因此，需要进行任务级别传输。



\subsection{网络流量特征分析}
随着数据中心中部署应用的增多，对数据中心流量研究工作也日益增多。
针对数据中心流量的特点，才能设计出高效的速率控制机制。
本部分对业界关于数据中心流量测量的工作进行了总结和归纳。

\textbf{短流}。数据中心中流大多是短流\cite{DCTCP}。
查询流非常短，通常对延迟十分敏感。
查询流和应用之前进行消息传递的流都是短流。
例如对于查询流，
当一个用户请求到达上层aggregator的节点时，上层的网络节点把请求下发给中层的节点，计算结果后，然后再下发给下层的worker节点。
worker节点得到请求后，读取请求的内容，然后计算结果，
随后把结果反馈给根节点，根节点根据得到的反馈结果，进行处理。
查询流的规模是非常有规律的，从高层的根节点到下层的worker节点的查询流的大小约为1.6KB，
从子节点回馈给根节点的流大小大约是1.6KB到2KB。



\textbf{长流}。长流和短流共同存在于数据中心。
大多数长流通常短于1MB，长的数目占据大约数据中心流总数目的不到1$\%$。
数据中心内也有很多长度在1MB到50MB的长流，这些流的数目不到数据中心流总数目的0.1$\%$。
但是数据中心超过$90\%$的数据量这些长流。
在分布式模型中，有很多拷贝和更新节点状态的背景流，大小在50KB到1MB。
背景流的到达间隔时间反映了应用的不同服务的叠加和应用的多样性。
背景流的特性为：
（1）流到达时间间隔跨度很大，到达是服从重尾分布，在较短的时间间隔内，会有大量背景数据流到达。
（2）长流发送的时间间隔有周期性，是因为下层的叶节点会周期性的更新文件，此过程中会产生大量长流。

\textbf{节点之间连接并行度}。
在数据中心内，节点之间连接并行度的中位数为36。
$99\%$的机器之间连接数目在1600以内。
当只考虑长流（$>$1MB）时，机器之间并发的长流数目较少，并行长流的平均数为1，
并且$75\%$的机器长流的并发数目在2以内。
这些长流，能持续几个RTT，交换机的缓冲区大部分空间都是由这些流的数据包占据。

总之，对延迟敏感的短流和需要高带宽的长流同时存在于数据中心，
这些数据流对网络资源的需求不同，
需要设计合理的网络协议，来满足不同的流对网络资源的需求。

\begin{figure}[b]
\begin{center}
\includegraphics [width=0.8\columnwidth] {figures/others/1.pdf}
\caption{数据中心数据流的期限分布}
\label{relatedwork-deadline-fig}
\end{center}
\end{figure}


\textbf{期限}。对于交互性和实时性应用，延迟是决定服务质量的关键。
研究表明，用户需要快速的回复响应\cite{kohavi2009controlled}。
应用通常具有200$\sim$300ms时间完成数据流传输并且进行计算\cite{Decandia2007Dynamo}，
因此需要尽快完成数据传输。

因此，诸如Partition-Aggregate这类模型的SLA (Service Level Agreement)
要求worker等节点的询问和应答传输在截止期限（deadline）之前完成。
任何没有在截止期限之前完成的数据流都被认定错失期限。
错失期限的数据流越多，会有越多的上层计算节点获得不完整的结果，进而影响服务的质量。
此外，在实际中不同的应用的数据流有不同的截止期限，甚至，同一个应用的不同阶段的传输的期限也不相同。
在实际中，数据中心的流包括时间敏感的短消息（50KB到1MB），用于更新节点状态和传输时间长的背景流量（5KB到50MB）等。
这些数据流在节点之间传递，不同业务的数据流具有不同的期限，有的甚至没有期限。
图\ref{relatedwork-deadline-fig}显示的流传输期限分布情形，从中发现，超过$90\%$的数据流的期限在50ms以内。
事实上，数据中心很多在线服务都需要性能保障\cite{Decandia2007Dynamo,Renesse2002Scalable}。
用户的请求，需要满足延迟的需求。
当请求到达时间节点时，
无论计算结果如何，节点都需要把结果反馈给用户，
如果错过了截止期限，应用的性能难以得到保障。



\subsection{TCP传输存在的问题}
TCP是一种面向连接的、可靠的、基于字节流的传输层通信协议。
尽管TCP在Internet中被广泛应用，然而数据中心应用常常需要数据中心网络提供低延迟高带宽。
因为数据中心网络资源有限，因此直接在数据中心使用TCP协议，会存在以下问题。

\subsubsection{Incast问题}
如果很多数据流在短时间内收敛在交换机的同一接口上，数据包可能会耗尽交换机的缓冲区，导致数据包丢失，导致Incast问题\cite{Vasudevan2009Safe}。
发生Incast常常是因为应用采取分区－聚合通信模式，因为在分区－聚合通信模式中，
工作节点常常在较短时间间隔内会发送反馈给上层节点，导致上层节点交换机缓冲区溢出。

迄今为止的实验发现，Incast经常会发生在实际生产环境中，Incast会影响用户体验，从而影响企业的效益。
这是因为，Incast会导致交换机缓冲区过大，从而请求的数据流错过期限。
如果使用TCP协议，则解决Incast问题的方法主要有：
（1）为每个请求增加随机抖动时间。
使用增加抖动时间的方法，可以避免Incast问题，
但是，给每条流增加随机的抖动，会使一些短流增加额外的延迟，从而使数据流错过期限；
（2）把数据流拆分成短流来适应交换机的缓冲区。
使用拆分数据流的方法，会从语意上破坏应用的完整性。
如果某条短流丢弃数据包，会使的整条长流传输不完整，从而破坏应用的性能，进而影响用户的体验。
\begin{figure}[b]
\begin{center}
\includegraphics [width=0.9\columnwidth] {figures/others/require.pdf}
\caption{数据中心应用的需求}
\label{relatedwork-require-fig}
\end{center}
\end{figure}
\subsubsection{交换机缓冲区队列过长}

大多数数据中心的交换机是共享内存，
交换机的所有端口共享交换机存储。
到达交换机接口的数据包被存储到所有接口共享的高速存储器中。
共享池中的内存由内存管理单元动态分配给数据包。
内存管理单元尝试为每个接口分配尽可能多的内存，
如果一个数据包必须排队等待出端口，
但接口已经达到了最大内存分配或者共享池本身已经耗尽，那么数据包就会被丢弃。
构建大型多端口存储器非常昂贵，
所以大多数便宜的交换机都是缓存都很小。

当前计算机网络判断拥塞的方法主要有三种：基于丢包的拥塞控制方法，基于RTT的拥塞控制方法，基于交换机队列长度的拥塞控制方法。
TCP使用基于丢包的拥塞控制方法，当网络中出现拥塞时，交换机的缓冲区会溢出，此时数据包会被丢弃。
当发送端发现有丢包产生时，拥塞窗口会减半，从而减小发送端速率。
如果不发生丢包，TCP的滑动窗口会一直增大，直到占满端口缓冲区。
因此使用TCP协议，会使的交换机缓冲区溢出并且交换机的队列过长，排队延迟高。
数据中心的传输延迟往往$<1ms$，而排队延迟大约是10ms\cite{DCTCP}，因此排队延迟是网络延迟的主要组成部分之一。
使用TCP会使排队延迟高，造成用户体验差，进而影响应用性能和收益。
因此在数据中心中，减小队列长度是减小延迟的有效方法。

\subsubsection{公平分配带来的问题}

数据中心并存了很多应用，这些应用对带宽和延迟的需求不同，使用TCP策略，带宽服从公平分配的原则。
然而，不同的应用对延迟和带宽需求不同，使用TCP，不能满足所有应用对带宽和延迟的需求。
图\ref{relatedwork-require-fig}显示的是数据中心应用对带宽和延迟的需求，横轴表示的是延迟，纵轴表示的是带宽，
在图\ref{relatedwork-require-fig}坐标系中，从左到右，延迟越来越大，从下到上带宽越来越大。
我们可以看到，对于Hadoop和数据备份，需要高带宽，对延迟需求不大。
对于PTP等应用，对带宽要求不高，但是需要低延迟。
使用TCP协议，对这些应用同等看待，难以满足应用的不同需求，因此，亟需新的方法，考虑应用的不同需求，
智能的分配网络资源，满足应用对网络资源的不同需求。


\section{新型数据中心网络应用传输方案}
日益使用的数据中心网络尽管有能给应用提供高带宽和低延迟的链路，但仍然面临频繁的网络拥塞\cite{Incast08, Incast09}。
传统的TCP协议，难以满足不同的应用对带宽和延迟的需求，针对TCP在数据中心性能不足的问题，业界提出了一系列的方法来弥补。
本部分对数据中心传输优化方法进行分类，同时对流级别和任务级别对传输优化方案进行综述。
\subsection{数据中心传输优化方法分类}
根据传输优化技术，可以分成分布式和集中式；根据传输优化粒度可以分成流级别传输和任务级别传输；
根据传输优化标准，有的设置了显式期限，有的优化传输完成时间；
根据传输体系结构，有的考虑路径有的把数据中心假设成非阻塞体系结构。
各类方案的对比如表\ref{methods-compare}所示。本文主要从传输优化粒度进行方案的分类和总结。
\begin{table}[h]
\centering
\caption{数据中心传输优化策略对比}\label{methods-compare}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
\setlength{\tabcolsep}{10pt}
分类原则& &\\ \hline
传输优化技术 &分布式（简单，控制精度差）&集中式（准确，增加延迟）\\ \hline
传输优化粒度 &流级别（需要信息少，不准确）&任务级别（需要信息多，准确）\\ \hline
传输优化标准 &显式期限（保证延迟，准确，期限难设）&传输完成时间（简单，不准确）\\ \hline
传输优化体系 &考虑路径（复杂，准确）&非阻塞结构（简单，准确度低）\\ \hline
\end{tabular}
\end{table}




\subsection{流级别传输优化方案}
流级别的调度方法，主要侧重提高截止期限前传输完成流的比例，以及优化流的平均完成时间。
从方法上看，流级别的传输优化方法主要可以分成两类，第一种是通过改进传输协议TCP等，调整滑动窗口来调整带宽；
第二种是进行传输调度，采用集中控制的方法，计算流在未来的一个时间段内的传输速率，使的流按照计算出的传输速率进行传输。
本小节，我们从流的传输方法上对流传输进行总结和归类。

\subsubsection{改进TCP的方案} 
在DCN中的许多改进TCP的传输方案中，DCTCP\cite{DCTCP}
提出在交换机的瞬时队列长度超过某个阈值K时，对数据包进行标记，
随后，接收端对被标记的数据包的ACK进行标记，发送端通过统计ACK被标记的比例来计算拥塞程度，
并根据拥塞程度（$\alpha$）来调整滑动窗口。
根据拥塞水平$\alpha$，出现拥塞时，滑动窗口变化为w = w$\times$（1 -$\alpha$/ 2）。
通过设置适当的阈值，DCTCP可以使的交换机的队列处于低水平上。
大量的实验表明，DCTCP有效地为短流提供低延迟服务。
然而DCTCP不是根据截止期限进行带宽调整的，
因此对于截止期限敏感的OLDI（Online-Data-Intensive)，MapReduce等应用（如网络查询和广告），
DCTCP的调整效果会很差 \cite{D2TCP}。

 D$^2$TCP改进了DCTCP，能更加智能的调整发送端的拥塞窗口。
 它定义了一个期限迫近因子d = T/D，其中T是不采用基于期限的策略进行传输所需要的时间，D是当前时刻到期限截止的剩余时间。
 出现拥塞时，根据网络拥塞程度$\alpha$和期限迫近因子d的拥塞窗口的调整方法：
$w=w \times (1-\alpha^{d}/2)$。
D$^2$TCP不但具有DCTCP保持排队长度稳定并且队列短的优良特性，
 而且可以使得期限更近的数据流获得更多的带宽，
 此外D$^2$TCP简单，易于在数据中心部署。
 
L$^2$DCT \cite{L2DCT} 尝试减小流完成时间，和D$^2$TCP类似，当出现拥塞时，
L$^2$DCT也使用伽玛校正函数进行滑动窗口调整($cwnd = cwnd*(1-\alpha^{w_c})$)。
L$^2$DCT 近似实现短流优先策略，小的数据流有更高的优先级，因此流平均完成时间变小。
和 D$^2$TCP，DCTCP一样，L$^2$DCT也需要借助网络中ECN的标记。
L$^2$DCT只能够优化流完成时间，而无法优化显式设置期限的数据流。
 
  
TIMELY\cite{mittal2015timely}是基于RTT判断拥塞的方法，
当RTT增大时，TIMELY认为网络中发生了拥塞，发送速率需要降低。
当RTT减小时，TIMELY认为网络拥塞程度轻，可以增大发送速率。
通过RTT进行拥塞判断，TIMELY可以使的交换机队列处于低水平上，从而有效的降低传输延迟。
但是，基于RTT的方法在数据中心存在以下两个问题：首先，数据中心网络中的RTT时间太短，无法准确测量；
而使用RDMA这些新型技术的成本太高。
此外，它不能和基于丢包的策略共存，
因此在实际当中部署存在难度。

ICTCP\cite{ICTCP}通过分析TCP带宽、RTT和TCP接收窗口（receive window）的关系，
动态的调整TCP接收端的接收窗口（receive window），从而把网络中出现Incast的影响尽量降低。
但是，数据中心RTT的波动大，难以进行测量；
其次，ICTCP侧重于优化网络中的Incast问题，
无法对背景流进行传输优化。

\inlinecite{Vulimiri2013Low} 将网络中对延迟要求很高的流复制，进行重复发送，借助网络中负载均衡机制，
可以降低因为链路部分拥塞而导致的短流延迟的增加。
使用短流复制的方法虽然可以有效的减小短流延迟，但是会增加链路的负载，并且会增加网络中Incast问题。

\subsubsection{流调度方案}
 
D$^{3}$ \cite{D3}在集中控制器上计算交换机上所需的带宽，
并把计算的带宽反馈给交换机，使交换机按照计算出的速率发送。
虽然D$^{3}$是第一个已知的基于截止期限进行带宽调整的传输策略，
但是由于其对数据流优先级调度采用贪心和先到先得的策略，
因此在某些情况下，期限远的数据流会比期限近的数据流优先调度 \cite{D3}。
另一方面，D$^{3}$ 需要修改交换机硬件，因此在实际中部署难度大。

PDQ \cite{PDQ}在端上使用抢占式流量调度来分配带宽。
与D$^{3}$不同，PDQ以分布式方式工作，
其抢占式调度可以处理D$^{3}$无法处理的场景。
PDQ可以使用不同的调度策略，如最早截止期限（Earliest Deadline First，简称EDF）或最短作业优先（Shortest Job First，简称SJF）。
然而，SJF是单一链路上的流平均完成时间最小化的最佳解决方案。
对于复杂链路来说，它并不是最优的。
PDQ使用流平均完成时间被用作评估指标，
PDQ还要求修改交换机的硬件，并且不能与TCP共存。
 
 PIAS\cite{bai2015information}借助于交换机当中存在的优先级队列， 近似实现了 MLFQ（Multiple Level Feedback Queue ）机制。
 在PIAS中，流的优先级开始最高，随着数据发送，
 流的优先级开始降低，PIAS近似实现了短流优先的策略，
 从而间接的优化了流平均完成时间。
 然而PIAS只是优化流平均完成时间，
 并没有考虑网络拥塞程度和流延迟之间的关系，
 当网络拥塞时，并不能满足紧急流对带宽和延迟的需求。
 
 
 
QJUMP\cite{grosvenor2015queues}给每个应用分配一个优先级，
对高优先级的应用在发送端进行限速，而一旦数据包进入网络中，
高优先级的数据包可以排在交换机的前列，低优先级的数据包排在交换机队列的后面。
从而实现了高优先级的数据包有较低的延迟，而低优先级的数据包，有更高的网络带宽。
QJUMP试图在高带宽和低延迟找到平衡。
但是，在实际应用中，应该合理的给应用分配优先级，
如果优先级分配不合理，会导致应用性能降低。
 
 
在FastPass \cite{perry2015fastpass}中，发送端何时发送数据包，
以及发送数据包的路径由中央处理器的计算结果决定。
FastPass借助数据包发送时间算法和数据包发送路径算法来决定数据数据包发送时间和发送路径。
通过这两个算法，FastPass能够在不改变硬件的前提下，
使交换机队列浅，从而降低排队延迟。
FastPass可以有效的降低网络中交换机队列长度，
但是FastPass是集中式控制的方法，集中式控制的方法在数据中心中会增加传输延迟。
此外，如果控制器发生故障，会导致系统崩溃。

pFabric\cite{pFabric}根据流剩余数据量大小进行调度，
pFabric\cite{pFabric}提出的分布式算法近似于最短作业优先（Shortest Job First，简称SJF）策略，
并且被证明在优化流平均完成时间问题上是近似最优策略。
与PDQ相比，pFabric将流调度与速率控制分离开，实现起来更加简单，并且可以与基于TCP的方案共存。
pFabric需要改动交换机，这导致pFabric在网络中难以部署。
 
 
DeTail\cite{DeTail}，侧重于跨层技术，
包括流的优先级调度和高效的负载平衡策略，
以减少长流的完成时间。
尽管能有效的减少网络传输延迟，但是DeTail需要使用新的网络层协议和传输层协议，
因此在实际难以大规模部署。


Karuna \cite{chen2016scheduling} 是混合流（有期限的流和没有期限的流混合）场景下的解决方案。
Karuna主张，对于有期限的流，要在截止期限之前完成，对于没有截止期限的流，流平均完成时间要尽可能短。
Karuna \cite{chen2016scheduling} 可以减少流失时间的流量百分比，并减少流的平均完成时间。 
Karuna需要在交换和终端节点上进行改变，这使得Karuna在实际中部署难度很大。

PASE\cite{PASE}声明的协议（DCTCP，PDQ，pFabric等）不是竞争关系，它们是互补的。
PASE不改变网络中的器件，在减少流完成时间方面性能要比pFabric略优。
 
\subsection{任务级别传输优化方案}
流级别的传输优化方案，能够优化单条流的完成时间，
或者尽量保证流在截止期限之前完成。
但是，单纯的流级别传输并不能满足应用的需求。
比如web 搜索（分区－聚合模型），数据备份，Map-Reduce，
分布式存储等应用是包含很多并行的数据流。
对多阶段的应用，只有当上一阶段传输完毕后，下一阶段才能开始传输，
因此，为了应用能够取得更好的性能，需要从流传输任务的级别进行优化。

Orchestra\cite{Chowdhury2011Managing}发现任务传输时间占据了任务总处理时间的33$\%$\cite{Chowdhury2011Managing}。
因此，Orchestra提出调度应该侧重优化任务级别的整体传输，而不只是优化某条数据流。
Orchestra使用一个全局的控制器来对所有传输信息进行同步，此全局控制器存储了所有源和目的节点信息。
对于广播的传输网络传输结构，Orchestra采用Cornet协议，更加合理的利用网络拓扑进行传输优化。
对于shuffle阶段的传输，Orchestra采用权重shuffle调度进行带宽分配。
Orchestra从总体上优化了任务传输，尤其对广播和Map-Reduce的shuffle阶段性能提升显著。

Barrat\cite{dogar2014decentralized}提出在调度应用的数据传输时，
应该在任务级别进行，而不是在流级别进行。
Barrat对任务的调度采用FIFO模式，对于数据量大的任务，
随着传输进行，任务优先级降低，从而保证大任务不会阻塞链路，影响小任务的传输。
Barrat是一个分布式的任务级别调度系统，
通过对任务的整体调度，Barrat能够有效的减小任务平均完成时间，同时减少任务最长的传输延迟，
进而提升应用的传输性能。


Varys\cite{chowdhury2014efficient}提出对于存在数据并行的应用，
应该以流组（coflow）\cite{chowdhury2012coflow}为调度单元，而仅仅在流级别进行优化是不够的。
Varys提出对应用的优化的目标是最小化平均流组完成时间。
为了达到这个目标，Varys采用集中式调度策略，首先计算流组的优先级，然后计算每条数据流的带宽。
对于流组优先级，Varys采用最小瓶颈优先（Smallest-Effective-Bottleneck-First，简称SEBF）启发式算法来决定。
最小瓶颈优先策略是把流组中传输最慢的流完成时间当做流组的预估完成时间，
然后根据预估传输时间进行排序，使得预估完成时间小的流组获得高优先级。
对于数据流的带宽的控制，使用最小带宽期望（Minimum-Allocation-for-Desired-Duration，简称MADD）算法。
使用集中控制的方法，借助以上两个策略，Varys优化了平均流组完成时间（Coflow Completion Time，简称CCT），从而提高了应用的性能。

Varys需要预先得知流组中流的大小等信息。
事实上，在数据中心中，应用的流大小信息是很难预先得知的，尤其在hadoop多阶段计算中，
中间shuffle会传输大量数据，数据流的大小很难被预先得知。
基于此，Aalo\cite{chowdhury2015efficient}提出，在调度流组时，应假设预先不知流组中流的大小进行调度。
Aalo实现了最短流组优先策略 ，
根据已经发送数据量的大小，把流组分配到不同的优先级队列中。
相同的优先级队列中的流组采取FIFO调度机制。
Aalo近似实现最短流组优先的策略，整体上调度了流组，从而优化了流组的传输效率。

事实上，无论Aalo还是Varys，都需要提前分别出应用的流组，这需要对应用进行改动。
而修正比如Hadoop和Spark等应用的API需要考虑兼容性和进行Java-byte级别的改动，这导致修改的难度较高。
CODA\cite{zhang2016coda}首先尝试不改变应用，来自动的区分和调度应用的流组。
同时使用late binding等措施来避免因流组区分错误而引发的资源利用率低等问题。
通过对应用的流组进行有效的区分和带宽的调度，
CODA从整体上对coflow实现了调度，从而，有效的降低了应用流组的传输时间，优化了应用传输延迟。


和Varys，以及Aalo等相同，D-CAS\cite{luo2016towards}将传输网络假设成非阻塞交换机，
认为传输冲突只发生在交换机的入端口以及出端口，中间路径并没有拥塞发生。
D-CAS提出最小化流组平均传输完成时间调度问题和开放商店中任务平均工作完成时间问题的优化等价。
为了优化流组传输延迟，D-CAS改进了开放商店中的任务平均完成时间的2-近似算法\cite{roemer2006note}，
并将2-近似调度算法引入到流组的调度中，然后改进2-近似调度策略，从而得到流组的在线调度算法。
D-CAS采用分布式的方法实现，可以有效的减小流组的传输时间。



RAPIER\cite{zhao2015rapier}打破了数据中心非阻塞体系结构，
认为在传输中，拥塞随时发生，不只发生在第一跳和最后一跳。
传输时不应只考虑调度问题，为每条数据流分配合理的路径是很重要的问题。
基于此，RAPIER把流组的路径选择和流组的调度放在一起考虑，
通过求解非整数优化问题，把每条数据流发送速率的大小和发送路径都计算出来。
RAPIER可以减少平均CCT在50$\%$以上，并且易于在真实环境中部署。
和Varys相同，RAPIER需要预先得知coflow中每条数据流大小等信息。


Task-aware TCP\cite{liu2017task}认为传统的TCP-based方法只考虑了单一流的传输，
而没有考虑流之间的关系，这会影响流之间的传输，导致传输效率低。
Task-aware TCP主张，在一个task中，速率过快的流应该把带宽让给速率慢的流。
Task-aware TCP\cite{liu2017task}改进了DCTCP，借助于ECN标记，区分出传输过快和传输过慢的流，
把传输过快的数据流带宽给传输过慢的数据流，
从而使的任务的平均传输时间减少，进而优化了任务的传输延迟。




论文\inlinecite{qiu2015minimizing}从理论角度考虑有权重的流组的调度，
假设已经知道coflow到达时间，每条流的信息进行调度。
Qiu同时强调调度流组时，应该也同时考虑权重，而不应该把所有流组同等看待。
使用近似算法，Qiu提出的策略，可以达到$\frac{67}{3}$的近似比。
使用随机性策略，算法的近似度可以达到$9+16\frac{\sqrt{2}}{3}$。
Qiu在\inlinecite{qiu2015minimizing}通过求解方程组对流组进行调度求解，此方法有较高的复杂度，
在实际中难以部署。




\subsection{新型数据中心传输方案存在的问题}

\begin{table}[h]
\centering
\caption{主要优化策略对比}\label{algorithm-compare}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
\setlength{\tabcolsep}{10pt}
方法&期限&应用完成时间&优化级别&考虑拥塞和性能关系&考虑任务重要性\\ \hline
DCTCP\cite{DCTCP}&&&流级别&$\times$&$\times$\\ \hline
D$^2$TCP\cite{D2TCP}&$\surd$&&流级别&$\times$&$\surd$\\ \hline
D$^3$\cite{D3}&$\surd$&&流级别&$\times$&$\surd$\\ \hline
PQD\cite{PDQ}&&$\surd$&流级别&$\times$&$\times$\\ \hline
pFabric\cite{pFabric}&&$\surd$&流级别&$\times$&$\times$\\ \hline
L$^2$DCT\cite{L2DCT}&&$\surd$&流级别&$\times$&$\times$\\ \hline
FastPass\cite{perry2015fastpass}&&$\surd$&流级别&$\times$&$\times$\\ \hline
Varys\cite{chowdhury2014efficient}&&$\surd$&任务级别&$\times$&$\times$\\ \hline
Aalo\cite{chowdhury2015efficient}&&$\surd$&任务级别&$\times$&$\times$\\ \hline
Karuna\cite{chen2016scheduling} &$\surd$&&流级别&$\times$&$\surd$\\ \hline
\end{tabular}
\end{table}


表\ref{algorithm-compare}展示的是数据中心当前主要传输优化策略对比情形。
可以发现，
相比于传统的TCP，当前数据中心传输方案能够有效的提高数据中心传输效率，
提高应用对网络等资源的使用情况，
然而，尽管数据中心网络技术在不断发展，
但是，随着应用业务的不断发展，数据中心网络始终难以满足应用的各种需求，
因此，需要更加合理的网络资源调度来满足应用对网络资源的需求。
当前数据中心网络依然存在以下的问题：

\textbf{（1）当前流级别的传输优化方案，没有考虑网络拥塞程度和流带宽区分度之间的关系，导致在重度拥塞时拥有不同优先级的流获得的带宽基本相同。}

D$^3$首先提出，在进行传输时，
应该给数据流分配足够多带宽，让数据流满足期限。
D$^3$改动交换机，并在交换机上计算出流传输满足期限的最小带宽，
从而流能够获得足够多的带宽，进而在流在截止期限之前完成。
D$^2$TCP根据网络的拥塞程度和流期限调整发送窗口，
通过拥塞窗口间接控制流带宽，从而期限近的流获得带宽多，期限远的流获得带宽少。
这两种方法虽然考虑了流的期限和网络拥塞之间的关系，
然而，无论哪种方法，都没有考虑网络拥塞程度和流带宽区分度之间的关系。
事实上，当网络拥塞程度严重时，D$^2$TCP不再是基于期限调整的拥塞控制策略。
因此，如何合理的根据网络拥塞程度、期限调整流传输是亟需解决的问题。


\textbf{（2）当前流级别传输方案，都侧重单独减少流错失期限的比例或者减小流平均完成时间，而并没有同时对二者优化。}

数据中心中同时存在对延迟敏感的短流和对带宽敏感的背景流。
当前的数据中心的传输方案，无论是优化有期限的流的传输方案－D$^2$TCP、
D$^3$还是优化流平均完成时间的方案－L$^2$DCT都是解决单一的需求：或者满足延迟敏感流的需求，或者满足带宽敏感流的需求，
并不能同时满足两种流的需求。
因此，需要新型的传输机制，能够同时对有期限的流和短流进行优化。

\textbf{（3）任务级别的传输调度，大多优化任务完成时间，并没有结合任务重要性进行调度，导致网络资源并没有合理的分配。}

尽管Varys，Aalo等策略在调度数据中心资源时，
是在任务级别进行的，
但是这些策略只是考虑网络的拥塞情形，
目标是优化任务的传输完成时间。
然而，应用有不同的重要性，只考虑网络拥塞是不够的，应该同时兼顾应用的重要性。
比如，进行计算的hadoop和视频传输业务共享数据中心资源，
进行计算的hadoop的业务的优先级比进行视频传输业务的重要程度高。
使用传统的传输策略，会认为这两个应用数据流的优先级相同，
这样常会导致进行计算的Hadoop业务的流得到的带宽不足，
从而Hadoop的完成时间过长，进而影响应用的整体性能并且造成网络资源利用率偏低。

基于以上几个问题，本文从流级别和任务级别对传输策略进行优化和改进，
从而在网络重度拥塞时，依然能够根据网络拥塞程度和流期限进行流带宽区分；
此外，提出应同时对有期限的流和短流同时做优化，
使有期限的流和对带宽敏感的流都能满足传输需求。
本文同时提出，在调度数据中心任务传输时，应同时兼顾网络拥塞和任务的重要性，
使网络资源得到更合理的分配。

\section{本章小结}
本章首先综述了数据中心网络中应用的编程模型，应用传输流的特征，期限特征等。
随后，总结了在数据中心内使用TCP传输存在的问题，以及从流级别和任务级别对TCP传输方案进行综述。
最后，本章总结了当前数据中心传输方案存在的问题。














